{"cells":[{"cell_type":"markdown","source":["#Testing our models\n","This file helps in creates our "],"metadata":{"id":"vUStt12hOo-2"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49855,"status":"ok","timestamp":1657191911770,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"oLRrE4Kz2I-G","outputId":"a071902e-85dc-4d9e-e8c6-c5d188bc933a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting SimpleITK\n","  Downloading SimpleITK-2.1.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n","\u001b[K     |████████████████████████████████| 48.4 MB 26 kB/s \n","\u001b[?25hInstalling collected packages: SimpleITK\n","Successfully installed SimpleITK-2.1.1.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from nibabel) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting skimage\n","  Downloading skimage-0.0.tar.gz (757 bytes)\n","\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/3b/ee/edbfa69ba7b7d9726e634bfbeefd04b5a1764e9e74867ec916113eeaf4a1/skimage-0.0.tar.gz#sha256=6c96a11d9deea68489c9b80b38fad1dcdab582c36d4fa093b99b24a3b30c38ec (from https://pypi.org/simple/skimage/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n","\u001b[31mERROR: Could not find a version that satisfies the requirement skimage (from versions: 0.0)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for skimage\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting medpy\n","  Downloading MedPy-0.4.0.tar.gz (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 28.8 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.21.6)\n","Requirement already satisfied: SimpleITK>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (2.1.1.2)\n","Building wheels for collected packages: medpy\n","  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for medpy: filename=MedPy-0.4.0-cp37-cp37m-linux_x86_64.whl size=754481 sha256=b202219363d00d04e97389d20fe32ae6b22ed49c3b95ac9a3369673abac53823\n","  Stored in directory: /root/.cache/pip/wheels/b0/57/3a/da1183f22a6afb42e11138daa6a759de233fd977a984333602\n","Successfully built medpy\n","Installing collected packages: medpy\n","Successfully installed medpy-0.4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting monai\n","  Downloading monai-0.9.0-202206131636-py3-none-any.whl (939 kB)\n","\u001b[K     |████████████████████████████████| 939 kB 28.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from monai) (1.21.6)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from monai) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->monai) (4.1.1)\n","Installing collected packages: monai\n","Successfully installed monai-0.9.0\n"]}],"source":["#\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!cp /content/drive/MyDrive/COLAB/DL_3D_MIA_project/metrics_acdc.py /content\n","\n","!pip install SimpleITK\n","!pip install nibabel\n","!pip install skimage\n","!pip install medpy\n","!pip install monai\n"]},{"cell_type":"code","source":["#Import all the required files\n","import SimpleITK as sitk\n","import nibabel as nib\n","import monai\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import os\n","import glob\n","\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from monai.transforms import LoadImage, SaveImage, Transform\n","from metrics_acdc import load_nii\n","from torch.nn.functional import pad\n","from monai.transforms.utils_pytorch_numpy_unification import maximum\n","from numpy import floor_divide\n","from monai.transforms import SaveImage, Spacing\n","from monai.data import create_test_image_3d, list_data_collate, decollate_batch\n","\n","systolic_mode = 'ES'\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# build dict of ACDC data\n","\n","model_path = '/content/drive/My Drive/COLAB/DL_3D_MIA_project/models/'\n","data_path = r'/content/drive/My Drive/COLAB/DL_3D_MIA_project/data/ACDC/testing'\n","\n","print(f'The used device is {device}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3-BP_CidyZR","executionInfo":{"status":"ok","timestamp":1657191919711,"user_tz":-120,"elapsed":7949,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}},"outputId":"976f6498-0d45-4fe0-9772-20645350c623"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The used device is cpu\n"]}]},{"cell_type":"markdown","metadata":{"id":"CijgGces2qIp"},"source":["# Construct the dictionary of file paths"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1657191919713,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"wTi5o7im2VfF","outputId":"6b917ab1-dd0f-4d88-8dcc-0b3c7a8e8dd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[56, 89, 27, 43, 70, 16, 41, 97, 10, 73, 12, 48, 86, 29, 94, 6, 67, 66, 36, 17, 50, 35, 8, 96, 28, 20, 82, 26, 63, 14, 25, 4, 18, 39, 9, 79, 7, 65, 37, 90, 57, 100, 55, 44, 51, 68, 47, 69, 62, 98, 80, 42, 59, 49, 99, 58, 76, 33, 95, 60, 64, 85, 38, 30, 2, 53, 22, 3, 24, 88, 92, 75, 87, 83, 21, 61, 72, 15, 93, 52]\n","[84, 54, 71, 46, 45, 40, 23, 81, 11, 1, 19, 31, 74, 34, 91, 5, 77, 78, 13, 32]\n","range(101, 151)\n"]}],"source":["# split of dataset in train and val must be based on patients not on image slices\n","train_patient, validation_patient = train_test_split(range(1,101), train_size=0.8, random_state=42)\n","test_patient = range(101, 151)\n","print(train_patient)\n","print(validation_patient)\n","print(test_patient)\n","\n","def build_dict_acdc(data_path, prediction_path='', mode='train'):\n","    \"\"\"\n","    This function returns a list of dictionaries, each dictionary containing the keys 'img' and 'mask' \n","    that returns the path to the corresponding image.\n","    \n","    Args:\n","        data_path (str): path to the root folder of the data set.\n","        mode (str): subset used. Must correspond to 'train', 'val' or 'test'.\n","        \n","    Returns:\n","        (List[Dict[str, str]]) list of the dictionaries containing the paths of images and masks.\n","    \"\"\"\n","    # test if mode is correct\n","    if mode not in [\"train\", \"val\", \"test\"]:\n","        raise ValueError(f\"Please choose a mode in ['train', 'val', 'test']. Current mode is {mode}.\")\n","    \n","    # define empty dictionary\n","    dicts = []\n","    if mode == \"test\":\n","        for patient_nb in list(test_patient):\n","            # set patient_path\n","            patient_path = os.path.join(data_path, 'patient'+'{0:03}'.format(patient_nb))\n","\n","            # extract ED and ES frame number from patient info\n","            patient_info = os.path.join(patient_path, 'Info.cfg')\n","            file = open(patient_info, 'r').readlines()\n","            ED_frame = int(file[0].split(':')[1])\n","            ES_frame = int(file[1].split(':')[1])\n","\n","            # set image and mask paths\n","            image_path = []\n","            image_path.append(os.path.join(patient_path, 'patient'+'{0:03}'.format(patient_nb)+'_frame'+'{0:02}'.format(ED_frame)+'.nii.gz'))\n","            image_path.append(os.path.join(patient_path, 'patient'+'{0:03}'.format(patient_nb)+'_frame'+'{0:02}'.format(ES_frame)+'.nii.gz'))\n","           \n","            mask_path = []\n","            dicts.append({'img': image_path, 'mask': mask_path})\n","    else:\n","        target = train_patient if mode == \"train\" else validation_patient \n","        # make a corresponding list for all the mask files\n","        for patient_nb in target:\n","            # set patient_path\n","            patient_path = os.path.join(data_path, 'patient'+'{0:03}'.format(patient_nb))\n","\n","            # extract ED and ES frame number from patient info\n","            patient_info = os.path.join(patient_path, 'Info.cfg')\n","            file = open(patient_info, 'r').readlines()\n","            ED_frame = int(file[0].split(':')[1])\n","            ES_frame = int(file[1].split(':')[1])\n","\n","            # set image and mask paths\n","            image_path = []\n","            image_path.append(os.path.join(patient_path, 'patient'+'{0:03}'.format(patient_nb)+'_frame'+'{0:02}'.format(ED_frame)+'.nii.gz'))\n","            image_path.append(os.path.join(patient_path, 'patient'+'{0:03}'.format(patient_nb)+'_frame'+'{0:02}'.format(ES_frame)+'.nii.gz'))\n","\n","            mask_path = []\n","            mask_path.append(os.path.join(patient_path, 'patient'+'{0:03}'.format(patient_nb)+'_frame'+'{0:02}'.format(ED_frame)+'_gt.nii.gz'))\n","            mask_path.append(os.path.join(patient_path, 'patient'+'{0:03}'.format(patient_nb)+'_frame'+'{0:02}'.format(ES_frame)+'_gt.nii.gz'))\n","            \n","            prediction_preprocessing_path = []\n","            \n","            if mode != \"test\":\n","                prediction_preprocessing_path.append(os.path.join(prediction_path,'predicted_segmentation_patient'+str(patient_nb)+'.npy'))\n","                prediction_preprocessing_path.append(os.path.join(prediction_path,'predicted_segmentation_patient'+str(patient_nb)+'.npy'))\n","\n","            dicts.append({'img': image_path, 'mask': mask_path, 'prediction_preprocessing': prediction_preprocessing_path})\n","    return dicts"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25929,"status":"ok","timestamp":1657191945631,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"wz1-S7eW3O78","outputId":"13bbebc7-db4b-4f37-defb-1512a14b0e28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of test dataset:  50\n"]}],"source":["test_dict_list = build_dict_acdc(data_path, mode='test')\n","print('Size of test dataset: ', len(test_dict_list))"]},{"cell_type":"markdown","metadata":{"id":"8jcmUDV1364C"},"source":["# Define a function that loads the middle slices (ED only) of the ACDC dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1657191945631,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"9eyEpMGm3kDh"},"outputs":[],"source":["class LoadACDCData(monai.transforms.Transform):\n","    \"\"\"\n","    This custom Monai transform loads the data from the acdc segmentation dataset.\n","    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n","    \"\"\"\n","    def __init__(self, keys=None):\n","        pass\n","\n","    def __call__(self, sample):\n","        q = 0 if systolic_mode == \"ED\" else 1\n","        image_q = load_nii(sample['img'][q])\n","        image = image_q[0][:,:,4]\n","        _, image_q_aff, image_q_header = load_nii(sample['img'][q])\n","        # remove third row and column to get affine matrix corresponding to 2D image\n","        image_q_aff = np.delete(image_q_aff, 2, 0)\n","        image_q_aff = np.delete(image_q_aff, 2, 1)\n","        # set pixel dimensions in image_ED_aff\n","        image_q_aff[0,0] = -image_q_header['pixdim'][1]\n","        image_q_aff[1,1] = -image_q_header['pixdim'][2]\n","\n","        return {'img': image, 'img_meta_dict': {'affine': image_q_aff, 'pixdim': image_q_header['pixdim']}}"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18773,"status":"ok","timestamp":1657191964395,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"DL0ovnW04EQ7","outputId":"a1f3a7d5-838d-4b40-f0fc-e31edb9601d9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading dataset: 100%|██████████| 50/50 [00:18<00:00,  2.68it/s]\n"]}],"source":["# create dataset with cache mechanism that can load data and cache deterministic transforms’ result during training\n","test_dataset = monai.data.CacheDataset(test_dict_list, transform=LoadACDCData())"]},{"cell_type":"markdown","metadata":{"id":"sI_B55uu5CFp"},"source":["# Define the transforms that should be applied on the test data\n","We add a channel in all of our data and perform intensitiy scaling with a lower bound of 5 and upper bound of 95.\n","Furthermore, all our images are sampled to the same spatial resolution of 1.5mm^2"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1657191964395,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"Pa5tAgwm42Xg"},"outputs":[],"source":["test_transform = monai.transforms.Compose(\n","    [\n","        LoadACDCData(),\n","        monai.transforms.AddChanneld(keys=['img']),\n","        monai.transforms.ScaleIntensityRangePercentilesd(keys=['img'], lower=5, upper=95, b_min=0, b_max=1),\n","        monai.transforms.Spacingd(keys=['img'], \n","                                  pixdim=(1.5, 1.5), \n","                                  mode=(\"bilinear\"))\n","    ]\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4145,"status":"ok","timestamp":1657191968877,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"67IihhFV5XKK","outputId":"44224480-54d9-46f1-ef76-7243565f57aa"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading dataset: 100%|██████████| 50/50 [00:04<00:00, 11.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Length of test_dataloader:  4\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["test_dataset_transformed = monai.data.CacheDataset(test_dict_list, transform=test_transform)\n","test_dataloader = monai.data.DataLoader(test_dataset_transformed, batch_size=16, shuffle=True)\n","print('Length of test_dataloader: ', len(test_dataloader))\n"]},{"cell_type":"markdown","metadata":{"id":"DxBgQNZhROcm"},"source":["## Calculate ROI for test set"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":2668,"status":"error","timestamp":1657191971531,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"zkgutMPELkDA","outputId":"6244fcc1-8c02-4062-a28a-0e4d01003d9a"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-6b46b5b41caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'20220615_1137_model_preprocess.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m             dtype=dtype)\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    137\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."]}],"source":["#This model is the pretrained ROI-Extracting model\n","model = monai.networks.nets.UNet(\n","    dimensions=2,\n","    in_channels=1,\n","    out_channels=1,\n","    channels = (16, 32, 64, 128, 256),\n","    strides=(2, 2, 2, 2),\n","    num_res_units=2,\n",").to(device)\n","\n","filename = '20220615_1137_model_preprocess.pt'\n","model.load_state_dict(torch.load(model_path+filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1852,"status":"aborted","timestamp":1657191971506,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"ZdUAxQS0-EKQ"},"outputs":[],"source":["#Several functions are defined here\n","\n","### Returns the largest connected component of a tensor\n","def getLargestCC(output):\n","    labels = skimage.measure.label(output)\n","    assert( labels.max() != 0 ) # assume at least 1 CC\n","    largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n","    return largestCC\n","\n","### Using a sliding window inference, the test images are masked using the ROI-extracting UNet\n","def visual_evaluation(sample, model):\n","    \"\"\"\n","    Allow the visual inspection the result of one sample by plotting \n","    the X-ray image, the ground truth (green) and the segmentation map produced by the network (red).\n","    \n","    Args:\n","        sample (Dict[str, torch.Tensor]): sample composed of an X-ray ('img') and a mask ('mask').\n","        model (torch.nn.Module): trained model to evaluate.\n","    \"\"\"\n","    model.eval()\n","    inferer = monai.inferers.SlidingWindowInferer(roi_size=[128,128], overlap=0.25, mode='gaussian')\n","    discrete_transform = monai.transforms.AsDiscrete(logit_thresh=0.5, threshold_values=True)\n","    Sigmoid = torch.nn.Sigmoid()\n","    with torch.no_grad():\n","        output = Sigmoid(inferer(sample['img'].to(device), network=model).cpu()).squeeze()\n","        output = discrete_transform(output)\n","        print(np.unique(output))\n","    \n","    output = getLargestCC(output)\n","    output = output.astype(int)\n","    ###IMPORTANT\n","    ###Calculate the center coordinates of the ROI by using the center of mass of the largest connected component.\n","    center_coordinates = scipy.ndimage.measurements.center_of_mass(output)\n","    center_coordinates = np.round(center_coordinates)   # round center_coordinates to get int values\n","    print('center: ', center_coordinates)\n","\n","    plt.figure()\n","    plt.imshow(output)\n","    plt.scatter(center_coordinates[1], center_coordinates[0], color='r')\n","    plt.show()\n","    \n","    fig, ax = plt.subplots(1,2, figsize = [12, 10])\n","    \n","    # Plot X-ray image\n","    ax[0].imshow(sample[\"img\"].squeeze(), 'gray')\n","    # Plot output\n","    overlay_output = np.ma.masked_where(output < 0.1, output >0.99)\n","    ax[1].imshow(sample['img'].squeeze(), 'gray')\n","    ax[1].imshow(overlay_output, 'Greens', alpha = 0.7, clim=[0,1])\n","    ax[1].set_title('Prediction')\n","    plt.show()\n","\n","    return output, center_coordinates"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1858,"status":"aborted","timestamp":1657191971512,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"nauOpdSmPgtJ"},"outputs":[],"source":["test_dict_list_evaluation = build_dict_acdc(data_path, mode='test')\n","test_transform_evalation = monai.transforms.Compose([\n","        LoadACDCData(),\n","        monai.transforms.AddChanneld(keys=['img']),\n","        monai.transforms.ScaleIntensityRangePercentilesd(keys=['img'], lower=5, upper=95, b_min=0, b_max=1),\n","        monai.transforms.Spacingd(keys=['img'], \n","                                  pixdim=(1.5,1.5), \n","                                  mode=['bilinear']),\n","    ]\n",")\n","test_dataset_evaluation = monai.data.CacheDataset(test_dict_list_evaluation, transform=test_transform_evalation)\n","test_dataloader_evaluation = monai.data.DataLoader(test_dataset_evaluation, batch_size=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1860,"status":"aborted","timestamp":1657191971514,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"q_M3NTJsNR6u"},"outputs":[],"source":["#Print out all the coordinates of the ROI-crop and store these in lists\n","output_test = []\n","center_coordinates_test = []\n","for i, sample in enumerate(test_dataloader_evaluation):\n","    print('patient number: ', test_patient[i])\n","    output_sample, center_sample = visual_evaluation(sample, model)\n","    output_test.append(output_sample)\n","    center_coordinates_test.append(center_sample)\n"]},{"cell_type":"markdown","metadata":{"id":"Tjdm1c7xP3j9"},"source":["#Segmentation\n","After having have received the coordinates for each of the Test patients heart location, we reload the dataset and load the model we want to test (Either UNET or DCNN for ES or ED).\n","\n","NB: Most explanations of the code are in the code fragments"]},{"cell_type":"markdown","metadata":{"id":"6mvOVgqiQD7Q"},"source":["## Reload the dataset \n","After we haveobtained the approximate coordinates for the heart, we will have to reload the dataset to include all slices. Then we can transform them with the SpatialCrop at the previously determined coordinates"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1861,"status":"aborted","timestamp":1657191971515,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"8ulhdKYzVSNO"},"outputs":[],"source":["class LoadACDCData(monai.transforms.Transform):\n","    \"\"\"\n","    This custom Monai transform loads the data from the acdc segmentation dataset.\n","    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n","    \"\"\"\n","    def __init__(self, keys=None):\n","        pass\n","\n","    def __call__(self, sample):\n","        q = 0 if systolic_mode == \"ED\" else 1\n","        image_q = load_nii(sample['img'][q])\n","        image = image_q[0][:,:,4]\n","        _, image_q_aff, image_q_header = load_nii(sample['img'][q])\n","        # remove third row and column to get affine matrix corresponding to 2D image\n","        image_q_aff = np.delete(image_q_aff, 2, 0)\n","        image_q_aff = np.delete(image_q_aff, 2, 1)\n","        # set pixel dimensions in image_ED_aff\n","        image_q_aff[0,0] = -image_q_header['pixdim'][1]\n","        image_q_aff[1,1] = -image_q_header['pixdim'][2]\n","\n","        return {'img': image, 'img_meta_dict': {'affine': image_q_aff, 'pixdim': image_q_header['pixdim']}}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1861,"status":"aborted","timestamp":1657191971515,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"IB3RX5j0VQNp"},"outputs":[],"source":["test_transform = monai.transforms.Compose(\n","    [\n","        LoadACDCData(),\n","        monai.transforms.AddChanneld(keys=['img']),\n","        monai.transforms.ScaleIntensityRangePercentilesd(keys=['img'], lower=5, upper=95, b_min=0, b_max=1),\n","        monai.transforms.Spacingd(keys=['img'], \n","                                  pixdim=(1.5, 1.5), \n","                                  mode=(\"bilinear\")),\n","    ]\n",")\n","\n","test_dataset_transformed = monai.data.CacheDataset(test_dict_list, transform=test_transform)"]},{"cell_type":"markdown","source":["In this block we store original data from the original images to recreate the images after we have transformed them and savely output them backtowards the NifTi format for evaluation on the ACDC website."],"metadata":{"id":"L-w5Oo2uUjyq"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1862,"status":"aborted","timestamp":1657191971516,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"rDFkB1KkbE-M"},"outputs":[],"source":["\n","i=0\n","original_3D_shapes = {}\n","original_3D_pixdims = {}\n","original_3D_affine = {}\n","for sample in test_dataset_transformed:\n","    patient_nb = test_patient[i]\n","    image = sample[\"img\"]\n","    pixdim = sample[\"img_meta_dict\"]['pixdim']\n","    affine = sample[\"img_meta_dict\"]['affine']\n","    original_3D_shapes[patient_nb] = list(image.shape)\n","    original_3D_pixdims[patient_nb] = pixdim\n","    original_3D_affine[patient_nb] = affine\n","    print(i, \": Image size: \", image.shape, \"Pixel Dimensions: \", pixdim\n","          # , '\\n Affine: \\n', affine\n","          )\n","    i=i+1\n","print(original_3D_shapes)"]},{"cell_type":"markdown","source":["The below function takes as input the test patient (3D)dataset and returns the slices cropped at the ROI coordinates."],"metadata":{"id":"FzJpBCsuVFbL"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1863,"status":"aborted","timestamp":1657191971517,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"Yc2d2HswP8QU"},"outputs":[],"source":["# change 3D dataset to 2D dataset\n","def change_dataset_from_3D_to_2D(dataset_3D, center_coordinates):\n","  samples = []\n","  for j, sample_3D in enumerate(dataset_3D):\n","    print(j)\n","    image_3D = sample_3D['img']\n","    print(image_3D.shape)\n","    for i in range(image_3D.shape[3]):\n","      print(center_coordinates)\n","      center_coordinates_i = center_coordinates[j]\n","      crop_transform = monai.transforms.SpatialCrop(roi_center=(int(center_coordinates_i[0]), int(center_coordinates_i[1])),\n","                                                    roi_size=(128, 128))\n","\n","      sample_2D = {}\n","      sample_2D['img'] = crop_transform(image_3D[:,:,:,i])\n","      sample_2D['img_meta_dict'] = sample_3D['img_meta_dict']\n","      samples.append(sample_2D)\n","\n","  return samples"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1864,"status":"aborted","timestamp":1657191971518,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"oY-99WRBTSTA"},"outputs":[],"source":["test_dataset_2D = change_dataset_from_3D_to_2D(test_dataset_transformed, center_coordinates_test)\n","print('Size of 2D testing dataset: ', len(test_dataset_2D))\n","\n","i=0\n","for sample in test_dataset_2D:\n","    i=i+1\n","    image = sample[\"img\"]\n","    meta = sample[\"img_meta_dict\"]\n","    print(i, \": Image size: \", image.shape, \"| meta:\", meta)"]},{"cell_type":"markdown","metadata":{"id":"PTXjZkzcQWnM"},"source":["## Load the segmentation model\n","The previously pre-trained Dilated CNN will be reloaded to give predictions on the testing images in 2D. After predictions, the slices will be stacked and transformed back to their original patients MRI 3D Image.\n","\n","NB: To test a different model, you must change the filename in the below code block."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1864,"status":"aborted","timestamp":1657191971519,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"2DqP0QM7QrzC"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","from typing import Any, Optional, Sequence, Tuple, Union\n","\n","from monai.networks.blocks.convolutions import Convolution, ResidualUnit, ADN\n","from monai.networks.layers.factories import Act, Norm\n","from monai.networks.layers.simplelayers import SkipConnection\n","from monai.utils import alias, deprecated_arg\n","\n","class CNN(nn.Module):\n","    \n","    def __init__(\n","        self,\n","        spatial_dims: int,\n","        in_channels: int,\n","        out_channels: int,\n","        kernel_size: Union[Sequence[int], int] = 3,\n","    ) -> None:\n","\n","        super().__init__()\n","\n","        self.conv1 = Convolution(\n","          spatial_dims=2,\n","          kernel_size=9,\n","          in_channels=1,\n","          out_channels=16,\n","          adn_ordering=\"NDA\",\n","          act=\"relu\",\n","          norm=\"batch\",\n","          dilation=1,\n","        )\n","        self.conv2 = Convolution(\n","          spatial_dims=2,\n","          kernel_size=7,\n","          in_channels=16,\n","          out_channels=64,\n","          adn_ordering=\"NDA\",\n","          act=\"relu\",\n","          norm=\"batch\",\n","          dilation=2,\n","        )\n","        self.conv3 = Convolution(\n","          spatial_dims=2,\n","          kernel_size=5,\n","          in_channels=64,\n","          out_channels=64,\n","          adn_ordering=\"NDA\",\n","          act=\"relu\",\n","          norm=\"batch\",\n","          dilation=4,\n","        )\n","        self.conv4 = Convolution(\n","          spatial_dims=2,\n","          kernel_size=3,\n","          in_channels=64,\n","          out_channels=64,\n","          adn_ordering=\"NDA\",\n","          act=\"relu\",\n","          norm=\"batch\",\n","          dilation=6,\n","        )\n","        self.conv5 = Convolution(\n","          spatial_dims=2,\n","          kernel_size=3,\n","          in_channels=64,\n","          out_channels=4,\n","          adn_ordering=\"NDA\",\n","          act=None,\n","          norm=\"batch\",\n","          dilation=8\n","        )\n","        self.final = torch.nn.Softmax()\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        #x = self.model(x)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.conv5(x)\n","        x = self.final(x)\n","\n","        return x\n","\n","model = CNN(\n","    spatial_dims=2,\n","    in_channels=1,\n","    out_channels=4,\n",").to(device)\n","\n","#TODO: Change this to the desired model\n","filename = '20220701_1526_model_main_DCNN_ED.pt'\n","model.load_state_dict(torch.load(model_path+filename))"]},{"cell_type":"markdown","metadata":{"id":"S_xw3xRwReNJ"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1652,"status":"aborted","timestamp":1657191971520,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"},"user_tz":-120},"id":"et6QfpTZeE2_"},"outputs":[],"source":["test_dataloader = monai.data.DataLoader(test_dataset_2D, batch_size=1)\n","print(len(test_dataloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM30MEwORhWQ","executionInfo":{"status":"aborted","timestamp":1657191971521,"user_tz":-120,"elapsed":1653,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"outputs":[],"source":["import time\n","model.eval()\n","results = []\n","for i, test_data in enumerate(test_dataloader):\n","    test_data = test_data[\"img\"].to(device)\n","    print(test_data.shape)\n","    inferer = monai.inferers.SimpleInferer()\n","    discrete_transform = monai.transforms.AsDiscrete(logit_thresh=0.5, threshold_values=True)\n","    with torch.no_grad():\n","        tic = time.perf_counter()\n","      \n","     \n","        output = inferer(inputs=test_data, network=model).cpu().squeeze()\n","        output = discrete_transform(output)\n","        toc = time.perf_counter()\n","        print(f\"Inference in {toc - tic:0.4f} seconds\")\n","    results.append(output)\n","  "]},{"cell_type":"markdown","metadata":{"id":"zY0E28O5QkjI"},"source":["## Save predictions\n","First we transform the predicted output of size 128x128 back into its original shape. Then we assign each patient their individual slices and resample our spatial resolution back towards the original spatial resolution provided in the meta data of the file. We must save the predictions to nii.gz format in order to compute the metrics on the ACDC Website."]},{"cell_type":"code","source":["nb_slices_per_patient_test=[]\n","\n","i=0\n","for sample in test_dataset_transformed:\n","    i=i+1\n","    image = sample[\"img\"]\n","    nb_slices = sample['img'].shape[3]\n","    nb_slices_per_patient_test.append(nb_slices)\n","    #print(i, \": Image size: \", image.shape)\n","\n","print(nb_slices_per_patient_test)\n"],"metadata":{"id":"T8uatrUrzleS","executionInfo":{"status":"aborted","timestamp":1657191971521,"user_tz":-120,"elapsed":1653,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Stack all the slices per patient to recreate the original models\n","output_test_3D = []\n","#mask_test_3D = []\n","index = 0\n","for i, patient_number in enumerate(test_patient):\n","  output_test_3D_i = torch.tensor(np.zeros([4,128,128,nb_slices_per_patient_test[i]]))\n","  for slice_i in range(nb_slices_per_patient_test[i]):\n","    output_test_3D_i[:,:,:,slice_i] = results[index]\n","    index=index+1\n","  output_test_3D.append(output_test_3D_i)\n","\n","print(len(output_test_3D))\n"],"metadata":{"id":"HExSvzbtQ4ov","executionInfo":{"status":"aborted","timestamp":1657191971521,"user_tz":-120,"elapsed":1653,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Retrieve all metadata in a list\n","meta = []\n","for test in test_dataset_transformed:\n","    meta.append(test['img_meta_dict'])\n","#print(meta)"],"metadata":{"id":"aLA7jCkk5QA6","executionInfo":{"status":"aborted","timestamp":1657191971522,"user_tz":-120,"elapsed":1654,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["segmented_patients = {}\n","for i in range(len(test_patient)):\n","    \n","    meta_data = meta[i]\n","    information = {}\n","    information['img'] = output_test_3D[i]\n","    information['img_meta_dict'] = meta_data\n","    segmented_patients[test_patient[i]] = information\n","    \n","print(segmented_patients[101]['img'].shape)\n","#All patients now have the segmented images and their original affine matrix after transformation\n","# #For each slice in the predicted segmentation create an empty multichannel tensor\n","# shape = (4, (original_shape[1]), (original_shape[2]))\n","# seg = torch.zeros(shape)\n","# #print(seg.shape)"],"metadata":{"id":"nSlbTbV0l_tx","executionInfo":{"status":"aborted","timestamp":1657191971524,"user_tz":-120,"elapsed":1656,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Recreates original shapes by padding the missing edges from the ROI coordinates\n","def recreate_original_matrix(original_shape, image, coordinates, roi_size=128):\n","    output = []\n","    channel = 0\n","    for i in image:\n","        padded_slices = []\n","        for j in range(image.shape[3]):\n","            to_pad = image[channel, :, :, j]\n","\n","            roi_start = coordinates - floor_divide(roi_size, 2)\n","            roi_end = maximum(roi_start + roi_size, roi_start)\n","            \n","            x, y = original_shape[1], original_shape[2]\n","            pad_directions = (int(roi_start[1]), int(y - roi_end[1]), int(roi_start[0]), int(x - roi_end[0]))\n","            #Pad background with missing 1s and all other classes with missing 0s\n","            mode = 1 if channel == 0 else 0\n","            padded_image = pad(to_pad, pad_directions, 'constant', mode)\n","            #print(\"New image shape:\", padded_image.shape)\n","            padded_slices.append(padded_image) \n","          \n","        padded_slices = torch.stack(padded_slices, dim=2)\n","        output.append(padded_slices)\n","        channel += 1\n","    output = torch.stack(output, dim=0)      \n","        \n","    print(output.shape)\n","    return output\n"," "],"metadata":{"id":"OPAJN8qAJVgF","executionInfo":{"status":"aborted","timestamp":1657191971525,"user_tz":-120,"elapsed":1657,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('/content/drive/My Drive/COLAB/DL_3D_MIA_project/masks')\n","saver = SaveImage(output_dir='/content/drive/My Drive/COLAB/DL_3D_MIA_project/masks', output_ext=\".nii.gz\", output_postfix='ED', separate_folder=False)\n","\n","index = 0\n","for nb, value in segmented_patients.items():\n","    original_shape = original_3D_shapes[nb]\n","    or_pixdim = original_3D_pixdims[nb]\n","    or_affine = original_3D_affine[nb] \n","    image = value['img']\n","    meta = value['img_meta_dict']\n","    #get the coordinates for this patient\n","    coordinates = center_coordinates_test[index]\n","    #Reformat to original spacing\n","    recreated_image = recreate_original_matrix(original_shape, image, coordinates)\n","    spacer = Spacing(pixdim=(or_pixdim[1], or_pixdim[2]), mode=\"bilinear\")\n","    recreated_image, new_affine, new_pixdim = spacer(recreated_image, affine=or_affine)\n","    #save to file\n","    fname = \"patient\" + str(nb)\n","    meta['filename_or_obj'] = fname\n","    fname = decollate_batch(meta['filename_or_obj'])\n","\n","    # plot predicted segmentation once\n","    if index == 0:\n","        for j in range(10):\n","          # best prediction\n","          fig = plt.figure(figsize=(24, 24))\n","\n","          plt.subplot(2, 4, 1)\n","          plt.imshow(np.transpose(recreated_image[1,:,:,j]), cmap='Reds')\n","          plt.imshow(np.transpose(recreated_image[2,:,:,j]), cmap='Greens', alpha = 0.4, clim=[0,1])\n","          plt.imshow(np.transpose(recreated_image[3,:,:,j]), cmap='Blues', alpha = 0.4, clim=[0,1])\n","          plt.subplot(2, 4, 2)\n","          plt.imshow(np.transpose(recreated_image[1,:,:,j]), cmap='Reds')\n","          # plt.imshow(np.transpose(mask_val_3D[12][1,:,:,3]), 'Reds', alpha = 0.2, clim=[0,1])\n","          plt.subplot(2, 4, 3)\n","          plt.imshow(np.transpose(recreated_image[2,:,:,j]), cmap='Greens')\n","          plt.subplot(2, 4, 4)\n","          plt.imshow(np.transpose(recreated_image[3,:,:,j]), cmap='Blues')\n","\n","    saver(recreated_image, meta)\n","    index += 1"],"metadata":{"id":"1IK5k1TIW_Ql","executionInfo":{"status":"aborted","timestamp":1657191971528,"user_tz":-120,"elapsed":1659,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Open the newly saved files and plot the image masks with the original image\n"],"metadata":{"id":"6vFmgzWYjUqH"}},{"cell_type":"code","source":["def plot_pred(or_image, seg):\n","    # plot predicted segmentation\n","    for i in range(or_image.shape[2]):\n","        plt.figure()\n","        plt.imshow(or_image[:,:,i].transpose(), 'gray')\n","        mask = seg[:,:,i,:]\n","        overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n","        plt.imshow(overlay_mask[:,:,1].transpose(), 'Reds', alpha=0.7, clim=[0,1], interpolation='nearest')\n","        plt.imshow(overlay_mask[:,:,2].transpose(), 'Greens', alpha=0.7, clim=[0,1], interpolation='nearest')\n","        plt.imshow(overlay_mask[:,:,3].transpose(), 'Blues', alpha=0.7, clim=[0,1], interpolation='nearest')\n","        plt.show()"],"metadata":{"id":"EISdvsv4bvAm","executionInfo":{"status":"aborted","timestamp":1657191971529,"user_tz":-120,"elapsed":1660,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["segmentation_path = r'/content/drive/My Drive/COLAB/DL_3D_MIA_project/masks/'\n","sample = segmentation_path + 'patient101_ED.nii.gz'\n","original = data_path + \"/patient101/patient101_frame01.nii.gz\"\n","\n","or_image, or_image_affine, or_image_header = load_nii(original)\n","image, image_affine, image_header = load_nii(sample)\n","\n","print(or_image.shape)\n","print(image.shape)\n","\n","plot_pred(or_image, image)\n","\n"],"metadata":{"id":"nxbYnlxL6Gz9","executionInfo":{"status":"aborted","timestamp":1657191971530,"user_tz":-120,"elapsed":1661,"user":{"displayName":"Colyn Jonker","userId":"18206108018939646328"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"project_test_dataset.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}